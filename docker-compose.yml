version: "3.9"

services:
  api:
    image: surakshaaithal241994/llm-summariser-service:latest
    build:
      context: ./fastAPI-backend
      dockerfile: Dockerfile
    container_name: summarizer-api
    environment:
      # Redis connection used by the FastAPI app
      - REDIS_URL=redis://redis:6379
      # Ollama base URL inside the compose network
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_API_URL=http://ollama:11434
      # Default model to use
      - OLLAMA_MODEL=gemma3:1b
    ports:
      - "8000:8000"
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis-data:/data
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      # Optional: predeclare a model name; model pulling is done on first use
      - OLLAMA_ORIGINS=*
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  # One-shot init container to ensure the model is pulled and hosted
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-lc", "ollama pull gemma3:1b"]
    restart: "no"

volumes:
  redis-data:
  ollama-data:


